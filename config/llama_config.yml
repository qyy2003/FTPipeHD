#Config File example
save_dir: logfile/llama
model_name: LLaMA
model_args:
  n_layer: 5
  #32
  n_head: 32
  n_embd: 4096
data:
  name: Alpaca
  path: ../data/alpaca
  batch_size: 8
schedule:
  #  resume:
  #  load_model: YOUR_MODEL_PATH
  optimizer:
    name: AdamW
    lr: 0.00003 #0.1
#    momentum: 0.9
#    weight_decay: 0.00004 #0.00004
  warmup:
    name: linear
    steps: 0 # 300
    ratio: 0.1
  total_epochs: 5
  lr_schedule:
    name: MultiStepLR
    milestones: [ 130,160,175,185 ] # [ 130,160,175,185 ]
    gamma: 0.1
  # lr_schedule:
  #   name: ExponentialLR
  #   gamma: 0.98
  val_intervals: 100 #100
log:
  interval: 10
weight_aggregation_interval: -1
