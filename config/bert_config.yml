#Config File example
save_dir: logfile/bert_for_classification
model_name: BERTForClassification
model_args:
  n_class: 9.0
  total_layer: 15.0
  hidden_size: 768.0
  vocab_size: 30522.0
  num_hidden_layers: 12.0
  attention_dropout_prob: 0.1
  hidden_dropout_prob: 0.1
  num_attention_heads: 12.0
  intermediate_size: 3072.0
weights_path: bert
data:
  name: conll2003
  path: ../data/conll2003
  batch_size: 1



  input_size: [3, 256, 256]
schedule:
  #  resume:
  #  load_model: YOUR_MODEL_PATH
  optimizer:
    name: AdamW
    lr: 0.00002 #0.1
#    momentum: 0.9
#    weight_decay: 0.00004 #0.00004
  warmup:
    name: linear
    steps: 0 # 300
    ratio: 0.1
  total_epochs: 5
  lr_schedule:
    name: MultiStepLR
    milestones: [ 130,160,175,185 ] # [ 130,160,175,185 ]
    gamma: 0.1
  # lr_schedule:
  #   name: ExponentialLR
  #   gamma: 0.98
  val_intervals: 100 #100
log:
  interval: 10
weight_aggregation_interval: -1
